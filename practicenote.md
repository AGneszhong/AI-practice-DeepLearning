### 1. 实例化两个标量

标量由只有一个元素的张量表示。 下面的代码将实例化两个标量
```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

print(x + y, x * y, x / y, x ** y)
```
#### 1.1 代码解释
代码解释：
导入PyTorch库。
创建两个张量（tensor）x和y，分别赋值为3.0和2.0。
打印x+y, x*y, x/y, x**y的结果。

详细解释torch.tensor：
在PyTorch中，tensor是一个多维数组，类似于NumPy的ndarray，但tensor可以在GPU上使用以加速计算。torch.tensor是PyTorch中用于存储和变换数据的主要工具。
torch.tensor()函数用于创建张量。它可以接受各种数据类型，如列表、元组、NumPy数组等，并可以指定张量的数据类型（dtype）和设备（device，如CPU或GPU）。
在上面的代码中，我们使用torch.tensor(3.0)和torch.tensor(2.0)创建了两个标量（0维张量）。然后对这两个标量进行了加法、乘法、除法和幂运算。

#### 2.1 输出结果
```
tensor(5.) tensor(6.) tensor(1.5000) tensor(9.)
```

### 2. 标量，向量，张量概念

一句话概括它们的关系：
**标量是最基本的单个数字，向量是一串有序的数字（即一维数组），而张量是向量的泛化，可以是任意维度的数组。在深度学习中，所有数据最终都以张量的形式表示和流动。**

你可以把它们想象成一个逐级扩展的家族：
*   **标量**：0维家庭成员，只有一个值。
*   **向量**：1维家庭成员，排成一队。
*   **矩阵**：2维家庭成员，排成一个方阵。
*   **张量**：3维及以上维度的家庭成员，可以排成更高维的结构（如立方体等）。**通常，我们习惯性地将标量、向量和矩阵也都统称为张量。**

#### 2.1 标量

*   **定义**：只有大小（数值），没有方向的量。它是一个单一的、独立的数值。
*   **维度**：0维
*   **深度学习中的例子**：
    *   **学习率**：一个非常重要的超参数，例如 `lr = 0.001`。
    *   **损失值**：模型预测与真实值之间的误差，例如 `loss = 2.3`。在每个训练步骤后，我们都会看到一个标量损失值。
    *   **模型的准确率**：例如 `accuracy = 0.95`。
*   **直观理解**：一个温度值（25°C），你的体重（65kg），一个简单的数字。

#### 2.2 向量

*   **定义**：一组**有序**的标量组成的集合。它既有大小，也有方向。可以看作是空间中的一个点或一个方向箭头。
*   **维度**：1维
*   **表示**：通常用粗体小写字母表示，如 **v**。`v = [v1, v2, v3, ..., vn]`
*   **深度学习中的例子**：
    *   **一个样本的特征向量**：例如，在房价预测中，一个房子的特征（面积、卧室数量、房龄）可以组成一个向量 `[120, 3, 10]`。
    *   **神经网络的单个全连接层**：一个神经元的输入就是上一层所有神经元输出的加权和，这个加权和就是一个向量点积的结果。
    *   **词向量**：在自然语言处理中，每个单词通常被表示成一个向量（例如50维或300维），向量中的每个数字代表了单词在某个语义维度上的值。
*   **直观理解**：平面或空间中的一个坐标点，如 `[2, 3]`。

#### 2.3 张量

*   **定义**：张量是标量和向量的**泛化**，可以理解为**一个容纳数据的容器，其维度可以任意定义**。向量是1维张量，矩阵是2维张量。
*   **维度**：可以是0维（标量）、1维（向量）、2维（矩阵）、3维、4维甚至更高。
*   **深度学习中的核心地位**：**深度学习框架（如PyTorch, TensorFlow）的基础数据结构就是张量。所有数据的输入、输出和转换都是在张量上进行的。**
*   **深度学习中的例子**：
    *   **0维张量**：如上所述的标量。
    *   **1维张量（向量）**：如上所述。
    *   **2维张量（矩阵）**：一个批量的样本数据。例如，一个包含32个房子信息的批量，每个房子有3个特征，那么数据就是一个 `[32, 3]` 的矩阵。
    *   **3维张量**：**彩色图片**。一张彩色图片有高度（H）、宽度（W）和颜色通道（C，通常是RGB三通道）。所以一张图片就是一个 `[C, H, W]` 或 `[H, W, C]` 的3维张量。
    *   **4维张量**：**一个批量的图片**。这是深度学习训练时最常见的数据形式。例如，一个批量包含64张图片，每张图片是3通道的224x224像素图片，那么这个批量就是一个 `[64, 3, 224, 224]` 的4维张量。
    *   **5维张量**：**视频数据**。视频可以看作是一系列图片的序列。其形状可能是 `[批量大小, 时间帧, 通道, 高度, 宽度]`。

### 3. 向量

向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。 
```python
import torch

x = torch.arange(4)
print(x)
print(x[3])
```

* import torch：导入PyTorch库，这是一个用于深度学习的开源库，提供了张量计算和动态神经网络。
* x = torch.arange(4)：创建一个一维张量（可以理解为数组）x，其中包含从0开始到3的四个整数（即0,1,2,3）。所以x是tensor([0, 1, 2, 3])。
* print(x)：打印整个张量x，输出将是：tensor([0, 1, 2, 3])
* print(x[3])：通过索引访问张量x的第四个元素（因为索引从0开始，所以索引3对应第四个元素），即3。所以输出将是：tensor(3)

#### 3.1 输出结果
```
tensor([0, 1, 2, 3])
tensor(3)
```

### 4. 长度、维度和形状

向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 在数学表示法中，如果我们想说一个向量`x`由`n`个实值标量组成， 可以将其表示为`x∈R^n`。 向量的长度通常称为**向量的维度**（dimension）。

与普通的Python数组一样，我们可以通过调用Python的**内置len()函数**来访问**张量的长度**。

```python
import torch

x = torch.arange(4)
print(x)
print(len(x))
```
使用**len(x)**来计算张量的长度。

**输出结果**
```
tensor([0, 1, 2, 3])
4
```
当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。

```python
import torch

x = torch.arange(4)
print(x)
print(x.shape)
```

使用**x.shape**来访问向量的长度。

**输出结果**

```
tensor([0, 1, 2, 3])
torch.Size([4])
```
向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

### 5.矩阵
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。
当调用函数来实例化张量时， 我们可以通过指定两个分量`m`和`n`来创建一个形状为的矩阵。

```python
import torch

A = torch.arange(20).reshape(5,4)
print(A)
```
`torch.arange(20)` 创建了一个从0到19（包括0，不包括20）的一维张量，即 [0, 1, 2, ..., 19]，总共20个元素。`.reshape(5,4)` 将这个一维张量重新塑形为一个5行4列的二维张量。重塑后的张量有5行，每行4个元素，因此总元素数仍然是20。然后将这个重塑后的张量赋值给变量A。最后打印A。
**输出结果**
```
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
```

#### 5.1 对称矩阵
一个**方阵**（行数和列数相等的矩阵）如果满足：
\[ A = A^T \]
即矩阵等于其转置矩阵，那么这个矩阵就是对称矩阵。

用元素表示就是：对于所有的 i 和 j，都有
\[ A_{ij} = A_{ji} \]

##### 5.1.1示例

```python
import torch

# 创建一个对称矩阵
A = torch.tensor([[1, 2, 3],
                  [2, 4, 5],
                  [3, 5, 6]])

print("对称矩阵 A:")
print(A)
print("A 的转置:")
print(A.T)
print("A 是否等于 A 的转置:", torch.allclose(A, A.T))
print(A==A.T)
```
`A.T`为A的转置,使用`torch.allclose`函数检查A和A的转置是否相等,`A==A.T`对A和A.T进行逐元素比较。
##### 5.1.2输出：
```
对称矩阵 A:
tensor([[1, 2, 3],
        [2, 4, 5],
        [3, 5, 6]])
A 的转置:
tensor([[1, 2, 3],
        [2, 4, 5],
        [3, 5, 6]])
A 是否等于 A 的转置: True
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
```
### 5.张量
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。 张量是描述具有任意数量轴的`n`维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。 
当我们开始处理图像时，张量将变得更加重要，图像以维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。

```python
import torch

X = torch.arange(24).reshape(2, 3, 4)
print(X)
```

**输出结果**
```
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

#### 5.1张量算法的基本性质
例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。

```python

```





































